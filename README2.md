# âš™ï¸ AskMe â€“ Setup & Usage Guide

This file explains how to install, run, and use the AskMe offline chatbot.

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/AskMe.git
cd AskMe
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Install Ollama
Download from: https://ollama.com

Run a model:
```bash
ollama run tinyllama
```

---

## â–¶ï¸ Run the Application

```bash
streamlit run app.py
```

Open in browser:  
http://localhost:8501

---

## ğŸ“„ How to Use

### Chat Mode
Type your message and get an instant offline response.

### PDF Question Answering (RAG)

1. Upload a PDF  
2. Ask questions like:  
   - â€œSummarize this documentâ€  
   - â€œWhat are the key points?â€  

The model answers using only the uploaded file.

---

## ğŸŒ Multilingual Example

User: à°¨à±à°µà±à°µà± à°à°µà°°à±?  
Bot: à°¨à±‡à°¨à± AskMe â€“ à°®à±€ à°†à°«à±â€Œà°²à±ˆà°¨à± AI à°¸à°¹à°¾à°¯à°•à±à°¡à±.

---

## ğŸ“ Project Structure

```
AskMe/
â”‚â”€â”€ app.py
â”‚â”€â”€ rag.py
â”‚â”€â”€ memory.py
â”‚â”€â”€ chat_history/
â”‚â”€â”€ models/
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README_1.md
â”‚â”€â”€ README_2.md
```

---

## ğŸ“œ License

MIT License

---

## â­ Resume Description

Developed AskMe, an offline multilingual AI chatbot using Ollama and TinyLlama with Streamlit UI, local PDF-based RAG using FAISS, chat memory, and multi-session history, running entirely without internet.
